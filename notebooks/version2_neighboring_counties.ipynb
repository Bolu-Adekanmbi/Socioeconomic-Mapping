{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0a6372e",
   "metadata": {},
   "source": [
    "## Required Inputs Before Running\n",
    "\n",
    "Ensure the following files are present in the `data/` directory (or adjust paths accordingly):\n",
    "\n",
    "1.  **`usa_pop_2022_CN_100m_R2025A_v1.tif`**: WorldPop Population Raster (100m resolution).\n",
    "2.  **`VIIRS_Region_2022.tif`**: VIIRS Nighttime Lights raster covering the full multi-county region. (Rename your local VIIRS file to this or update the path in the code).\n",
    "3.  **`Sentinel2_Region_2022.tif`**: Sentinel-2 Cloud-Free Composite raster covering the full multi-county region. (Rename your local Sentinel file to this or update the path in the code).\n",
    "4.  **`roads.gpkg`** (Optional): If you already have a road network file, otherwise the script will attempt to download it via OSMnx.\n",
    "\n",
    "**Note:** The script will generate `ground_truth_neighboring.csv` and various GeoJSON files automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04caf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import contextily as ctx\n",
    "from shapely.geometry import Point, Polygon, box\n",
    "from shapely.ops import unary_union\n",
    "import censusdata\n",
    "import osmnx as ox\n",
    "import rioxarray as rxr\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "from rasterstats import zonal_stats\n",
    "\n",
    "# Configuration\n",
    "DATA_DIR = \"data\"\n",
    "ALIGNED_DIR = \"data/aligned_data\"\n",
    "OUTPUT_DIR = \"outputs\"\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(ALIGNED_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# CRS Settings\n",
    "# EPSG:4326 for storage/lat-lon\n",
    "# EPSG:32617 (UTM Zone 17N) for metric calculations in Virginia\n",
    "CRS_LATLON = \"EPSG:4326\"\n",
    "CRS_PROJECTED = \"EPSG:32617\" \n",
    "\n",
    "# County FIPS Codes for Virginia (State FIPS 51)\n",
    "COUNTIES = {\n",
    "    \"Montgomery\": \"121\",\n",
    "    \"Craig\": \"045\",\n",
    "    \"Roanoke\": \"161\",\n",
    "    \"Floyd\": \"063\",\n",
    "    \"Pulaski\": \"155\",\n",
    "    \"Giles\": \"071\",\n",
    "    \"Radford City\": \"750\"\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ac4a3d",
   "metadata": {},
   "source": [
    "## 1. Data Collection: Boundaries & Block Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082a1e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Download County Boundaries\n",
    "print(\"Downloading County Boundaries...\")\n",
    "try:\n",
    "    counties_gdf = gpd.read_file(\"https://www2.census.gov/geo/tiger/TIGER2023/COUNTY/tl_2023_us_county.zip\")\n",
    "    \n",
    "    # Filter for our target counties in Virginia (State 51)\n",
    "    target_fips = list(COUNTIES.values())\n",
    "    region_counties = counties_gdf[\n",
    "        (counties_gdf[\"STATEFP\"] == \"51\") & \n",
    "        (counties_gdf[\"COUNTYFP\"].isin(target_fips))\n",
    "    ].copy()\n",
    "    \n",
    "    # Create a unified boundary for the region\n",
    "    region_boundary_geom = unary_union(region_counties.geometry)\n",
    "    region_boundary = gpd.GeoDataFrame(geometry=[region_boundary_geom], crs=region_counties.crs)\n",
    "    \n",
    "    # Save\n",
    "    region_counties.to_file(os.path.join(ALIGNED_DIR, \"region_counties.geojson\"), driver=\"GeoJSON\")\n",
    "    region_boundary.to_file(os.path.join(ALIGNED_DIR, \"region_boundary.geojson\"), driver=\"GeoJSON\")\n",
    "    \n",
    "    print(f\"Loaded {len(region_counties)} counties.\")\n",
    "    region_counties.plot()\n",
    "    plt.title(\"Target Region Counties\")\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error downloading boundaries: {e}\")\n",
    "\n",
    "# 1.2 Download Block Groups\n",
    "print(\"Downloading Block Groups...\")\n",
    "try:\n",
    "    bg_gdf = gpd.read_file(\"https://www2.census.gov/geo/tiger/TIGER2023/BG/tl_2023_51_bg.zip\")\n",
    "    \n",
    "    # Filter for target counties\n",
    "    region_bgs = bg_gdf[\n",
    "        (bg_gdf[\"STATEFP\"] == \"51\") & \n",
    "        (bg_gdf[\"COUNTYFP\"].isin(target_fips))\n",
    "    ].copy()\n",
    "    \n",
    "    # Save\n",
    "    region_bgs.to_file(os.path.join(ALIGNED_DIR, \"block_groups_neighboring.geojson\"), driver=\"GeoJSON\")\n",
    "    print(f\"Loaded {len(region_bgs)} block groups.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error downloading block groups: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fab4eb9",
   "metadata": {},
   "source": [
    "## 2. Data Collection: ACS Ground Truth (Income, Population, Poverty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd31b680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables to download\n",
    "# B19013_001E: Median Household Income\n",
    "# B01003_001E: Total Population\n",
    "# B17021_002E: Income in the past 12 months below poverty level\n",
    "# B17021_001E: Total population for whom poverty status is determined\n",
    "\n",
    "variables = [\n",
    "    'B19013_001E', # Median Household Income\n",
    "    'B01003_001E', # Total Population\n",
    "    'B17021_002E', # Poverty Count\n",
    "    'B17021_001E'  # Poverty Total\n",
    "]\n",
    "\n",
    "dfs = []\n",
    "\n",
    "print(\"Downloading ACS Data...\")\n",
    "for county_name, county_fips in COUNTIES.items():\n",
    "    try:\n",
    "        print(f\"  Fetching data for {county_name} ({county_fips})...\")\n",
    "        county_data = censusdata.download(\n",
    "            'acs5', \n",
    "            2022,\n",
    "            censusdata.censusgeo([\n",
    "                ('state', '51'), \n",
    "                ('county', county_fips), \n",
    "                ('block group', '*')\n",
    "            ]),\n",
    "            variables\n",
    "        )\n",
    "        dfs.append(county_data)\n",
    "    except Exception as e:\n",
    "        print(f\"  Error fetching {county_name}: {e}\")\n",
    "\n",
    "if dfs:\n",
    "    acs_data = pd.concat(dfs)\n",
    "    \n",
    "    # Helper to extract GEOID\n",
    "    def extract_geoid(index):\n",
    "        # censusdata index is an object with params() method\n",
    "        params = dict(index.params())\n",
    "        return params['state'] + params['county'] + params['tract'] + params['block group']\n",
    "\n",
    "    acs_data['GEOID'] = acs_data.index.map(extract_geoid)\n",
    "    \n",
    "    # Process Columns\n",
    "    # Income\n",
    "    acs_data['median_income'] = acs_data['B19013_001E'].where(acs_data['B19013_001E'] >= 0)\n",
    "    \n",
    "    # Population\n",
    "    acs_data['population'] = acs_data['B01003_001E'].where(acs_data['B01003_001E'] >= 0)\n",
    "    \n",
    "    # Poverty Rate\n",
    "    poverty_total = acs_data['B17021_001E'].where(acs_data['B17021_001E'] > 0)\n",
    "    poverty_count = acs_data['B17021_002E'].where(acs_data['B17021_002E'] >= 0)\n",
    "    acs_data['poverty_rate'] = poverty_count / poverty_total\n",
    "    \n",
    "    # Select final columns\n",
    "    ground_truth = acs_data[['GEOID', 'median_income', 'population', 'poverty_rate']].copy()\n",
    "    \n",
    "    # Save\n",
    "    ground_truth.to_csv(os.path.join(DATA_DIR, \"ground_truth_neighboring.csv\"), index=False)\n",
    "    print(f\"Saved ground truth data with {len(ground_truth)} records.\")\n",
    "    print(ground_truth.head())\n",
    "else:\n",
    "    print(\"No ACS data downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ddca08",
   "metadata": {},
   "source": [
    "## 3. Data Collection: OpenStreetMap (OSM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5238ec80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the unified region boundary for OSM queries\n",
    "region_poly = region_boundary.geometry.iloc[0]\n",
    "\n",
    "# 3.1 Healthcare\n",
    "print(\"Fetching Healthcare facilities...\")\n",
    "try:\n",
    "    healthcare = ox.features_from_polygon(\n",
    "        region_poly,\n",
    "        tags={\"amenity\": [\"hospital\", \"clinic\", \"doctors\", \"pharmacy\"]}\n",
    "    )\n",
    "    healthcare = healthcare.reset_index(drop=True)\n",
    "    healthcare = healthcare[['geometry', 'amenity', 'name']] # Keep relevant cols\n",
    "    healthcare.to_file(os.path.join(ALIGNED_DIR, \"healthcare_neighboring.geojson\"), driver=\"GeoJSON\")\n",
    "    print(f\"  Fetched {len(healthcare)} healthcare facilities.\")\n",
    "except Exception as e:\n",
    "    print(f\"  Error fetching healthcare: {e}\")\n",
    "\n",
    "# 3.2 Schools\n",
    "print(\"Fetching Schools...\")\n",
    "try:\n",
    "    schools = ox.features_from_polygon(\n",
    "        region_poly,\n",
    "        tags={\"amenity\": [\"school\", \"college\", \"university\"]}\n",
    "    )\n",
    "    schools = schools.reset_index(drop=True)\n",
    "    schools = schools[['geometry', 'amenity', 'name']]\n",
    "    schools.to_file(os.path.join(ALIGNED_DIR, \"schools_neighboring.geojson\"), driver=\"GeoJSON\")\n",
    "    print(f\"  Fetched {len(schools)} schools.\")\n",
    "except Exception as e:\n",
    "    print(f\"  Error fetching schools: {e}\")\n",
    "\n",
    "# 3.3 Grocery (NO FILTERING)\n",
    "print(\"Fetching Grocery stores...\")\n",
    "try:\n",
    "    grocery = ox.features_from_polygon(\n",
    "        region_poly,\n",
    "        tags={\"shop\": [\"supermarket\", \"convenience\", \"grocery\"]}\n",
    "    )\n",
    "    grocery = grocery.reset_index(drop=True)\n",
    "    # Keep all columns or just relevant ones? Let's keep geometry and shop type\n",
    "    grocery = grocery[['geometry', 'shop', 'name']]\n",
    "    grocery.to_file(os.path.join(ALIGNED_DIR, \"grocery_neighboring.geojson\"), driver=\"GeoJSON\")\n",
    "    print(f\"  Fetched {len(grocery)} grocery stores.\")\n",
    "except Exception as e:\n",
    "    print(f\"  Error fetching grocery: {e}\")\n",
    "\n",
    "# 3.4 Roads\n",
    "print(\"Fetching Road Network (this may take a while)...\")\n",
    "roads_path = os.path.join(DATA_DIR, \"roads.gpkg\")\n",
    "if not os.path.exists(roads_path):\n",
    "    try:\n",
    "        # Download graph\n",
    "        G = ox.graph_from_polygon(region_poly, network_type='drive')\n",
    "        # Convert to GeoDataFrame\n",
    "        nodes, edges = ox.graph_to_gdfs(G)\n",
    "        # Save edges as roads\n",
    "        edges.to_file(roads_path, driver=\"GPKG\")\n",
    "        print(f\"  Fetched {len(edges)} road segments.\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error fetching roads: {e}\")\n",
    "else:\n",
    "    print(\"  Roads file already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7ec888",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdc61a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Block Groups and Reproject for Distance Calculations\n",
    "bgs = gpd.read_file(os.path.join(ALIGNED_DIR, \"block_groups_neighboring.geojson\"))\n",
    "bgs = bgs.to_crs(CRS_PROJECTED)\n",
    "\n",
    "# Calculate Centroids\n",
    "bgs['centroid'] = bgs.geometry.centroid\n",
    "\n",
    "# Initialize Feature Matrix with GEOID\n",
    "features = bgs[['GEOID']].copy()\n",
    "\n",
    "# Helper function for OSM features\n",
    "def calculate_osm_features(gdf_path, name_prefix, block_groups_gdf):\n",
    "    print(f\"Processing {name_prefix}...\")\n",
    "    if not os.path.exists(gdf_path):\n",
    "        print(f\"  Warning: {gdf_path} not found. Skipping.\")\n",
    "        return pd.DataFrame({'GEOID': block_groups_gdf['GEOID']})\n",
    "        \n",
    "    poi_gdf = gpd.read_file(gdf_path).to_crs(CRS_PROJECTED)\n",
    "    \n",
    "    # 1. Count of POIs within Block Group\n",
    "    # Spatial Join\n",
    "    joined = gpd.sjoin(poi_gdf, block_groups_gdf, how=\"inner\", predicate=\"intersects\")\n",
    "    counts = joined.groupby(\"GEOID\").size().reset_index(name=f\"{name_prefix}_count\")\n",
    "    \n",
    "    # 2. Distance to Nearest POI\n",
    "    # Use centroids of block groups\n",
    "    # For efficiency, use nearest_points or cKDTree if dataset is large, \n",
    "    # but for this scale, simple distance matrix or apply might work.\n",
    "    # Let's use geometry.distance to the unary_union of POIs (or nearest)\n",
    "    \n",
    "    # Better approach for distance:\n",
    "    # For each BG centroid, find distance to nearest POI geometry\n",
    "    # Using apply with distance to the whole POI set is slow.\n",
    "    # Using cKDTree on POI centroids is faster.\n",
    "    \n",
    "    poi_centroids = poi_gdf.geometry.centroid\n",
    "    \n",
    "    def get_min_dist(point):\n",
    "        return poi_centroids.distance(point).min()\n",
    "        \n",
    "    # Note: This can still be slow. \n",
    "    # Optimization: Use sindex.nearest\n",
    "    \n",
    "    min_dists = []\n",
    "    for geom in block_groups_gdf.centroid:\n",
    "        # Find nearest index\n",
    "        nearest_idx = list(poi_gdf.sindex.nearest(geom, return_all=False))[1][0]\n",
    "        nearest_geom = poi_gdf.iloc[nearest_idx].geometry\n",
    "        dist = geom.distance(nearest_geom)\n",
    "        min_dists.append(dist)\n",
    "        \n",
    "    dist_df = pd.DataFrame({\n",
    "        'GEOID': block_groups_gdf['GEOID'],\n",
    "        f'{name_prefix}_nearest_dist': min_dists\n",
    "    })\n",
    "    \n",
    "    return counts.merge(dist_df, on=\"GEOID\", how=\"outer\")\n",
    "\n",
    "# 4.1 Calculate OSM Features\n",
    "# Healthcare\n",
    "health_feats = calculate_osm_features(os.path.join(ALIGNED_DIR, \"healthcare_neighboring.geojson\"), \"health\", bgs)\n",
    "features = features.merge(health_feats, on=\"GEOID\", how=\"left\")\n",
    "\n",
    "# Schools\n",
    "school_feats = calculate_osm_features(os.path.join(ALIGNED_DIR, \"schools_neighboring.geojson\"), \"school\", bgs)\n",
    "features = features.merge(school_feats, on=\"GEOID\", how=\"left\")\n",
    "\n",
    "# Grocery\n",
    "grocery_feats = calculate_osm_features(os.path.join(ALIGNED_DIR, \"grocery_neighboring.geojson\"), \"grocery\", bgs)\n",
    "features = features.merge(grocery_feats, on=\"GEOID\", how=\"left\")\n",
    "\n",
    "# Fill NaNs in counts with 0 (distance NaNs should remain if no POIs found, but here we likely found some)\n",
    "features[[c for c in features.columns if 'count' in c]] = features[[c for c in features.columns if 'count' in c]].fillna(0)\n",
    "\n",
    "print(\"OSM Features Calculated.\")\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80f122f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Road Density\n",
    "print(\"Calculating Road Density...\")\n",
    "roads_path = os.path.join(DATA_DIR, \"roads.gpkg\")\n",
    "if os.path.exists(roads_path):\n",
    "    roads = gpd.read_file(roads_path, layer='edges').to_crs(CRS_PROJECTED)\n",
    "    \n",
    "    # Spatial Join to assign road segments to block groups\n",
    "    # This splits roads at BG boundaries if we use overlay, but sjoin just checks intersection.\n",
    "    # For density, we ideally want length of roads *within* the BG.\n",
    "    # Overlay is expensive. \n",
    "    # Approximation: Clip roads to BGs?\n",
    "    \n",
    "    # Let's use overlay intersection for accurate length\n",
    "    roads_clipped = gpd.overlay(roads, bgs[['GEOID', 'geometry']], how='intersection')\n",
    "    roads_clipped['length_m'] = roads_clipped.length\n",
    "    \n",
    "    road_stats = roads_clipped.groupby('GEOID')['length_m'].sum().reset_index()\n",
    "    road_stats.rename(columns={'length_m': 'road_length_m'}, inplace=True)\n",
    "    \n",
    "    # Calculate Density (m per sq km)\n",
    "    bgs['area_sqkm'] = bgs.geometry.area / 10**6\n",
    "    road_stats = bgs[['GEOID', 'area_sqkm']].merge(road_stats, on='GEOID', how='left').fillna(0)\n",
    "    road_stats['road_density'] = road_stats['road_length_m'] / road_stats['area_sqkm']\n",
    "    \n",
    "    features = features.merge(road_stats[['GEOID', 'road_density']], on='GEOID', how='left')\n",
    "    print(\"Road Density Calculated.\")\n",
    "else:\n",
    "    print(\"Roads file not found. Skipping density.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc53f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 Raster Features (Population, VIIRS, Sentinel)\n",
    "\n",
    "def calculate_raster_stats(raster_path, name_prefix, block_groups_gdf, stats=['mean', 'median']):\n",
    "    print(f\"Processing {name_prefix} from {raster_path}...\")\n",
    "    if not os.path.exists(raster_path):\n",
    "        print(f\"  Warning: {raster_path} not found. Skipping.\")\n",
    "        return None\n",
    "        \n",
    "    # Read raster\n",
    "    # We use rasterstats which handles projection matching if we pass the affine transform?\n",
    "    # Actually rasterstats expects the raster and vector to be in the same CRS or the raster path to be handled.\n",
    "    # Best practice: Reproject vector to raster CRS.\n",
    "    \n",
    "    with rasterio.open(raster_path) as src:\n",
    "        raster_crs = src.crs\n",
    "        raster_transform = src.transform\n",
    "        raster_array = src.read(1)\n",
    "        nodata = src.nodata\n",
    "        \n",
    "    # Reproject BGs to Raster CRS\n",
    "    bgs_raster_crs = block_groups_gdf.to_crs(raster_crs)\n",
    "    \n",
    "    # Calculate Stats\n",
    "    stats_list = zonal_stats(\n",
    "        bgs_raster_crs,\n",
    "        raster_path,\n",
    "        stats=stats\n",
    "    )\n",
    "    \n",
    "    stats_df = pd.DataFrame(stats_list)\n",
    "    stats_df.columns = [f\"{name_prefix}_{c}\" for c in stats_df.columns]\n",
    "    stats_df['GEOID'] = block_groups_gdf['GEOID'].values\n",
    "    \n",
    "    return stats_df\n",
    "\n",
    "# Population\n",
    "pop_path = os.path.join(DATA_DIR, \"usa_pop_2022_CN_100m_R2025A_v1.tif\")\n",
    "pop_stats = calculate_raster_stats(pop_path, \"pop\", bgs, stats=['sum', 'mean'])\n",
    "if pop_stats is not None:\n",
    "    features = features.merge(pop_stats, on=\"GEOID\", how=\"left\")\n",
    "\n",
    "# VIIRS Nightlights\n",
    "viirs_path = os.path.join(DATA_DIR, \"VIIRS_Region_2022.tif\")\n",
    "viirs_stats = calculate_raster_stats(viirs_path, \"viirs\", bgs, stats=['mean', 'max'])\n",
    "if viirs_stats is not None:\n",
    "    features = features.merge(viirs_stats, on=\"GEOID\", how=\"left\")\n",
    "\n",
    "# Sentinel-2 (NDVI/Bands)\n",
    "# Assuming the Sentinel file is a composite or specific band. \n",
    "# If it's a multi-band raster, we might want stats for specific bands.\n",
    "# For now, assuming single band or just taking stats of the first band (often RGB or composite).\n",
    "# If the user has a multi-band file, they might need to adjust this to read specific bands.\n",
    "sentinel_path = os.path.join(DATA_DIR, \"Sentinel2_Region_2022.tif\")\n",
    "sentinel_stats = calculate_raster_stats(sentinel_path, \"sentinel\", bgs, stats=['mean', 'std'])\n",
    "if sentinel_stats is not None:\n",
    "    features = features.merge(sentinel_stats, on=\"GEOID\", how=\"left\")\n",
    "\n",
    "print(\"Raster Features Calculated.\")\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1589b21c",
   "metadata": {},
   "source": [
    "## 5. Consolidation and Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bb56f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with Ground Truth\n",
    "ground_truth = pd.read_csv(os.path.join(DATA_DIR, \"ground_truth_neighboring.csv\"))\n",
    "# Ensure GEOID is string\n",
    "ground_truth['GEOID'] = ground_truth['GEOID'].astype(str)\n",
    "features['GEOID'] = features['GEOID'].astype(str)\n",
    "\n",
    "master_matrix = features.merge(ground_truth, on=\"GEOID\", how=\"left\")\n",
    "\n",
    "# Handle Missing Values\n",
    "# Drop rows where target (median_income) is missing?\n",
    "# Or keep them for prediction?\n",
    "# For training, we need the target.\n",
    "print(f\"Total Block Groups: {len(master_matrix)}\")\n",
    "print(f\"Block Groups with Income Data: {master_matrix['median_income'].notna().sum()}\")\n",
    "\n",
    "# Save\n",
    "output_file = \"master_feature_matrix_neighboring_counties.csv\"\n",
    "master_matrix.to_csv(output_file, index=False)\n",
    "print(f\"Successfully saved {output_file}\")\n",
    "\n",
    "# Preview\n",
    "print(master_matrix.info())\n",
    "master_matrix.head()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
